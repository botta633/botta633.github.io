<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FS Cache Performance: Bigger FSCache size impacts the performance! - Botta's Blog</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../image-styles.css">
</head>

<body>
<header class="header">
    <div class="logo">Code Chronicles</div>
    <nav class="nav">
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="#">About</a></li>
            <li><a href="software.html">Software Blog</a></li>
            <li><a href="life.html">Life Blog</a></li>
            <li><a href="#">Contact</a></li>
        </ul>
    </nav>
</header>

<main class="article-page">
    <section class="article-hero">
        <div class="article-category">Software Playbook</div>
        <h1>Oh Cache!: How bigger FSCache size impacts the performance!</h1>
        <p class="article-subtitle">
            A practical experiment showing how file-system cache size can impact the performance of an app.
        </p>
        <div class="article-meta">
            <span class="meta-pill">Published: December 26, 2025</span>
            <span class="meta-pill">Reading time: 15 minutes</span>
            <span class="meta-pill">Stack: Linux / perf / strace</span>
        </div>
        <div class="article-hero-actions">
            <a href="software.html" class="button-cyber">Back to Software Blog</a>
        </div>
    </section>

    <section class="article-content">
        <!-- Executive Summary -->
        <div class="content-block">
            <h2 class="section-title">Executive Summary</h2>
            <p>
                I ran a controlled benchmark on Linux to test a claim from Brendan Gregg’s performance work:
                as the file system cache grows in number of records, performance can actually get worse.
                By reading a fixed 8&nbsp;GiB file using different record sizes (4&nbsp;KiB → 1&nbsp;MiB) and
                profiling with <code>strace</code> and <code>perf</code>, I showed that:
            </p>
            <ul>
                <li>64&nbsp;KiB records were the “sweet spot” for my machine.</li>
                <li>4&nbsp;KiB records made the job almost 2× slower, even though the same total bytes were read.</li>
                <li>The extra cost came from kernel page-cache management and syscall overhead, not from disk I/O.</li>
            </ul>
            <p>
                This post walks through the experiment design, the tools, and the kernel functions that showed up
                in the profiles (<code>filemap_get_pages</code>, <code>xas_load</code>, <code>__filemap_add_folio</code>,
                <code>copy_page_to_iter</code>, <code>_copy_to_iter</code>, etc.).
            </p>
        </div>

        <!-- Problem Statement -->
        <div class="content-block">
            <h2 class="section-title">Problem Statement</h2>
            <p>
                The question I wanted to answer was:
                <strong>“Why did file system performance degrade as the FS cache grew in size (in records)?”</strong>
                In other words, if the total data size is fixed but we make each record smaller, we create more
                cache entries. At some point, is the overhead of managing all those entries worse than the benefit
                of finer granularity?
            </p>
            <p>
                To make this concrete, I wanted numbers for:
            </p>
            <ul>
                <li>Wall-clock time to read 8&nbsp;GiB.</li>
                <li>Total syscall time and syscall counts from <code>strace -c</code>.</li>
                <li>CPU cycles and hot kernel functions from <code>perf record</code>/<code>perf report</code>.</li>
            </ul>

            <div class="callout-grid">
                <div class="callout-card">
                    <h3>Primary Objective</h3>
                    <p>
                        Demonstrate that shrinking record size (and therefore increasing the number of cached
                        records) can make performance <em>worse</em>, and tie this to concrete kernel functions
                        in the page-cache path.
                    </p>
                </div>
                <div class="callout-card">
                    <h3>Guardrails</h3>
                    <p>
                        Keep the workload simple and reproducible:
                        sequential binary file, no application logic, single binary compiled with <code>gcc -O2</code>,
                        run on a quiet machine with page cache dropped between runs.
                    </p>
                </div>
                <div class="callout-card">
                    <h3>Timeline</h3>
                    <p>
                        One evening of implementation and scripting, plus another evening of profiling,
                        interpretation, and writing this post.
                    </p>
                </div>
            </div>
        </div>

        <!-- Architecture / Flow -->
        <div class="content-block">
            <h2 class="section-title">Architecture / Flow</h2>
            <p>
                The “architecture” here is intentionally small:
                a single benchmark binary, a single 8&nbsp;GiB file, and a loop that repeatedly calls
                <code>pread()</code> with a configurable record size.
                Around that, I wrapped <code>strace</code> and <code>perf</code> via a small Python
                orchestration script to collect syscall and CPU profiles.
            </p>

            <p>
                Conceptually, each run looks like this:
            </p>
            <ol>
                <li>Ensure the page cache is clean (e.g., <code>sync</code> + <code>echo 3 &gt; /proc/sys/vm/drop_caches</code>).</li>
                <li>Run <code>fs_bench</code> with a given <code>--record-size</code> through the Python script.</li>
                <li>Inside the script, wrap the run with <code>strace -c</code> to summarize syscall cost.</li>
                <li>Also wrap it with <code>perf stat</code> and <code>perf record -g</code> for deeper profiling.</li>
            </ol>
            <p>
                Inside the kernel, each <code>pread()</code> call follows roughly this path:
                <code>pread → vfs_read → filemap_read → filemap_get_pages → xas_load / __filemap_add_folio
                → copy_page_to_iter / _copy_to_iter</code>. The interesting part is how often these functions are
                called when we use 4&nbsp;KiB vs 64&nbsp;KiB records.
            </p>
        </div>

        <!-- How I Simulated a Bigger FS Cache -->
        <div class="content-block">
            <h2 class="section-title">How I Simulated a “Bigger” FS Cache</h2>

            <p>
                I didn't manually resize the Linux page cache. Instead, I made the cache
                effectively <em>larger in number of entries</em> by shrinking the record size of
                each read.
            </p>

            <p>
                The total data size was always fixed at <strong>8&nbsp;GiB</strong>. What changes is how
                that data is sliced. When I use 1&nbsp;MiB records, the kernel only needs to track
                about 8,000 chunks. When I use 4&nbsp;KiB records, the same file explodes into
                more than <strong>2 million</strong> chunks. Each chunk becomes a separate folio/XArray
                entry inside the page cache.
            </p>

            <div class="callout-grid">
                <div class="callout-card">
                    <h3>Big records</h3>
                    <p>Fewer <code>pread()</code> calls → fewer cache entries → less metadata overhead.</p>
                </div>
                <div class="callout-card">
                    <h3>Tiny records</h3>
                    <p>Millions of <code>pread()</code> calls → millions of entries → lots of metadata work.</p>
                </div>
            </div>

            <p>
                So instead of “resizing” the FS cache, I changed the workload in a way that
                forced the kernel to manage more cached objects — the same real-world effect
                databases and file processing tools hit when they use very small I/O sizes.
            </p>
        </div>

        <!-- Reproducing the Experiment -->
        <div class="content-block">
            <h2 class="section-title">Reproducing the Experiment</h2>

            <p>
                Everything starts with a single 8&nbsp;GiB binary file and a tiny benchmark binary.
                I generated the data file once:
            </p>

            <div class="code-block">
                <pre><code># Create an 8 GiB file (pick your poison: dd or fallocate)
dd if=/dev/urandom of=data.bin bs=1M count=8192
# or:
# fallocate -l 8G data.bin</code></pre>
            </div>

            <p>
                Then I compiled the benchmark program:
            </p>

            <div class="code-block">
                <pre><code>gcc -O2 -Wall -o fs_bench fs_bench.c</code></pre>
            </div>

            <p>
                Instead of typing <code>strace</code> / <code>perf</code> commands by hand for each record size,
                I wrote a small Python script (<code>run_recordsize_experiment.py</code>) to orchestrate everything.
                At the top of the script I define the record sizes and workload:
            </p>

            <div class="code-block">
<pre><code>FS_BENCH = "./fs_bench"
DATA_FILE = "data.bin"

TOTAL_BYTES = 8 * 1024**3   # 8 GiB workload

RECORD_SIZES = [
    1024 * 1024,   # 1 MiB
    256 * 1024,    # 256 KiB
    64  * 1024,    # 64 KiB
    16  * 1024,    # 16 KiB
    4   * 1024,    # 4 KiB
]

MODE = "rand"
SEED = 12345</code></pre>
            </div>

            <p>
                For each record size, the script builds a base command for the benchmark:
            </p>

            <div class="code-block">
<pre><code>base_cmd = [
    FS_BENCH,
    "--file", DATA_FILE,
    "--mode", MODE,
    "--record-size", str(record_size),
    "--total-bytes", str(TOTAL_BYTES),
    "--seed", str(SEED),
]</code></pre>
            </div>

            <p>
                Then it runs three things in sequence:
            </p>

            <div class="code-block">
<pre><code># 1) plain run to measure wall time (in Python)
proc, wall_time = run_simple(base_cmd)

# 2) strace -c to summarize syscall cost
s_proc = subprocess.run(
    ["strace", "-c"] + base_cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True,
)

# 3) perf stat to collect hardware counters
p_proc = subprocess.run(
    ["perf", "stat", "-x", ",", "-e", ",".join(PERF_EVENTS), "--"] + base_cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True,
)</code></pre>
            </div>

            <p>
                The script parses the text output of <code>strace -c</code> and <code>perf stat</code>,
                extracts totals for syscall counts and time plus selected perf events (cycles, instructions,
                cache misses, major/minor faults, context switches), and appends a row into
                <code>results.csv</code>:
            </p>

            <div class="code-block">
<pre><code>writer.writerow([
    record_size,
    TOTAL_BYTES,
    MODE,
    f"{wall_time:.6f}",
    strace_calls,
    f"{strace_time:.6f}",
    int(cycles),
    int(instr),
    int(cache_misses),
    int(majflt),
    int(minflt),
    int(cs),
])</code></pre>
            </div>

            <p>
                The end result is the CSV you see in the repo, with rows like:
            </p>

            <div class="code-block">
<pre><code>record_size,total_bytes,mode,wall_time_sec,strace_syscalls,strace_syscall_time_sec,...
1048576,8589934592,rand,1.141898,16454,1.274368,...
262144,8589934592,rand,0.735765,65606,1.206744,...
65536,8589934592,rand,0.680724,262216,1.501656,...
16384,8589934592,rand,0.766102,1048648,2.292304,...
4096, 8589934592,rand,1.302721,4194376,6.138182,...</code></pre>
            </div>

            <div class="image-grid">
                <figure class="image-figure">
                    <img src="fs_bench_time_vs_record_size.png"
                         alt="fs_bench_time_vs_record_size">
                    <figcaption>
                        This figure shows how very small record sizes (representing bigger cache size) 
                        and how very big ones are bad as well. 64KB seems to be the sweet spot for this machine.
                </figure>

            <p>
                From there I used a tiny plotting script to create the record-size graphs, and I kept two
                <code>perf record</code> outputs (4&nbsp;KiB and 64&nbsp;KiB) to generate flame graphs.
                Large raw perf data and intermediate log files were deleted later to keep the directory small;
                what remains are:
            </p>

            <ul>
                <li><code>results.csv</code> – the summary table</li>
                <li><code>fs_bench_record_4k.png</code>, <code>fs_bench_record_64k.png</code> – plots from that CSV</li>
                <li><code>flame_4k.svg</code>, <code>flame_64k.svg</code> – flame graphs from the perf data</li>
            </ul>
        </div>

        <!-- Implementation Notes -->
        <div class="content-block">
            <h2 class="section-title">Implementation Notes</h2>
            <p>
                The benchmark program (<code>fs_bench</code>) is intentionally tiny: it opens a file,
                then issues <code>pread()</code> calls in a loop until <code>total_bytes</code> have been read.
                Record size and access pattern (<code>seq</code> vs <code>rand</code>) are command-line arguments.
            </p>
            <ul>
                <li>
                    <strong>Phase 1 — Benchmark binary.</strong>
                    Implemented in C with a simple loop around <code>pread()</code>. No timing inside the
                    binary to keep it easy to profile with external tools.
                </li>
                <li>
                    <strong>Phase 2 — Experiment harness.</strong>
                    A Python script that:
                    <ul>
                        <li>loops over record sizes,</li>
                        <li>runs <code>fs_bench</code> with the same total bytes every time,</li>
                        <li>wraps each run in <code>strace -c</code> and <code>perf stat</code>,</li>
                        <li>logs results into <code>results.csv</code>.</li>
                    </ul>
                </li>
                <li>
                    <strong>Phase 3 — Analysis.</strong>
                    Plotted record size vs wall-time and syscall time, then correlated those with
                    hot functions in <code>perf report</code> (e.g., <code>filemap_get_pages</code>,
                    <code>xas_load</code>, <code>__filemap_add_folio</code>, <code>copy_page_to_iter</code>,
                    <code>_copy_to_iter</code>).
                </li>
            </ul>
        </div>

        <!-- Key Metrics / Signals -->
        <div class="content-block">
            <h2 class="section-title">Key Metrics / Signals</h2>
            <p>
                All runs read the same 8&nbsp;GiB file; only the record size changed.
                Numbers below are representative from one profiling session and are plotted in these two screenshots.
            </p>

            <div class="image-grid">
                <figure class="image-figure">
                    <img src="fs_bench_record_4k.png"
                         alt="Record size vs wall time and syscall metrics (4 KiB highlighted)">
                    <figcaption>
                        Record-size experiment overview: the negative test point at 4&nbsp;KiB clearly stands out
                        with much higher wall time and syscall time for the same 8&nbsp;GiB of data.
                    </figcaption>
                </figure>

                <figure class="image-figure">
                    <img src="fs_bench_record_64k.png"
                         alt="Record size vs wall time and syscall metrics with 64 KiB as sweet spot">
                    <figcaption>
                        Zoom on the “sweet spot”: 64&nbsp;KiB records deliver the best wall time on this machine,
                        balancing syscall overhead and page-cache work.
                    </figcaption>
                </figure>
            </div>

            <div class="callout-grid">
                <div class="callout-card">
                    <h3>Before (large records)</h3>
                    <p>
                        1&nbsp;MiB records: ~1.14&nbsp;s wall time, ~16k syscalls. Throughput is okay,
                        but not optimal on this machine.
                    </p>
                    <span class="pill">Baseline</span>
                </div>
                <div class="callout-card">
                    <h3>After (sweet spot at 64&nbsp;KiB)</h3>
                    <p>
                        64&nbsp;KiB records: ~0.68&nbsp;s wall time, ~262k syscalls.
                        This was the best-performing configuration and served as the “good”
                        reference point for the negative test.
                    </p>
                    <span class="pill">Best Result</span>
                </div>
                <div class="callout-card">
                    <h3>Negative Test (4&nbsp;KiB)</h3>
                    <p>
                        4&nbsp;KiB records: ~1.30&nbsp;s wall time, ~4.1M syscalls, ~6.1&nbsp;s of total syscall time.
                        <code>perf report</code> and the flame graph below show heavy time in page-cache lookup,
                        folio insertion, and data copying, confirming that tiny records can make the FS cache
                        expensive to manage.
                    </p>
                    <span class="pill">Experiment Signal</span>
                </div>
            </div>

            <h3>Flame Graphs: 4&nbsp;KiB vs 64&nbsp;KiB</h3>
            <p>
                To visualize <em>where</em> the CPU is burning cycles, I turned the <code>perf record</code> data
                into flame graphs. The x-axis is proportional to total time; a wider bar means more time spent in
                that function (including its children).
            </p>

            <div class="flamegraphs-grid">
                <figure class="flamegraph-figure">
                    <img src="flame_4k.svg" alt="Flame graph for fs_bench with 4 KiB records">
                    <figcaption>
                        4&nbsp;KiB records: note how wide the
                        <code>_copy_to_iter</code> / <code>copy_page_to_iter</code> block is.
                        Because we issue ~4&nbsp;million <code>pread()</code> calls, the kernel spends a lot of
                        absolute time copying data from the page cache into user buffers.
                    </figcaption>
                </figure>

                <figure class="flamegraph-figure">
                    <img src="flame_64k.svg" alt="Flame graph for fs_bench with 64 KiB records">
                    <figcaption>
                        64&nbsp;KiB records: the <code>_copy_to_iter</code> region is noticeably narrower.
                        Even if its <em>percentage</em> of the run is similar or slightly higher, the overall run
                        is much shorter, so the <strong>total time</strong> spent in <code>_copy_to_iter</code>
                        is lower than in the 4&nbsp;KiB case. Fewer syscalls and fewer cache entries mean less work
                        for the kernel per 8&nbsp;GiB.
                    </figcaption>
                </figure>
            </div>

            <p>
                This pair of flame graphs makes the story visible: with 4&nbsp;KiB records we push the kernel into
                doing a huge number of small copies and page-cache lookups, so functions like
                <code>_copy_to_iter</code>, <code>copy_page_to_iter</code>, <code>filemap_get_pages</code>, and
                <code>__filemap_add_folio</code> occupy much more total time. With 64&nbsp;KiB records, the same
                8&nbsp;GiB of data is covered by ~16× fewer operations, and the hot blocks in the flame graph
                shrink accordingly.
            </p>
        </div>

        <!-- Code Reference -->
        <div class="content-block">
            <h2 class="section-title">Code Reference</h2>
            <p>
                This section shows the critical pieces: the user-space benchmark loop, and a simplified view of
                the kernel's page-cache lookup path we saw in <code>perf report</code>.
            </p>

            <h3>User-space benchmark (<code>fs_bench</code>)</h3>
            <div class="code-block">
<pre><code><span class="code-comment">// fs_bench.c (simplified)</span>
<span class="code-keyword">static</span> <span class="code-type">void</span> <span class="code-function">run_benchmark</span>(<span class="code-type">int</span> fd, <span class="code-type">size_t</span> record_size, <span class="code-type">off_t</span> total_bytes) {
    <span class="code-type">char</span> <span class="code-operator">*</span>buf = <span class="code-function">aligned_alloc</span>(<span class="code-number">4096</span>, record_size);
    <span class="code-keyword">if</span> (<span class="code-operator">!</span>buf) { <span class="code-function">perror</span>(<span class="code-string">"alloc"</span>); <span class="code-function">exit</span>(<span class="code-number">1</span>); }

    <span class="code-type">off_t</span> offset = <span class="code-number">0</span>;
    <span class="code-type">off_t</span> read_bytes = <span class="code-number">0</span>;

    <span class="code-keyword">while</span> (read_bytes <span class="code-operator">&lt;</span> total_bytes) {
        <span class="code-type">ssize_t</span> n = <span class="code-function">pread</span>(fd, buf, record_size, offset);
        <span class="code-keyword">if</span> (n <span class="code-operator">&lt;</span> <span class="code-number">0</span>) {
            <span class="code-function">perror</span>(<span class="code-string">"pread"</span>);
            <span class="code-keyword">break</span>;
        }
        <span class="code-keyword">if</span> (n <span class="code-operator">==</span> <span class="code-number">0</span>) {
            <span class="code-comment">// EOF - wrap around or stop, depending on mode</span>
            <span class="code-keyword">break</span>;
        }

        read_bytes <span class="code-operator">+=</span> n;
        offset     <span class="code-operator">+=</span> n; <span class="code-comment">// sequential mode; random mode picks offset differently</span>
    }

    <span class="code-function">free</span>(buf);
}</code></pre>
            </div>
            <p>
                The important part for FS cache behavior is that <strong>only record size changes</strong>.
                Total bytes (<code>total_bytes</code>) stay constant at 8&nbsp;GiB, so shrinking
                <code>record_size</code> means "more operations for the same data".
                In the 4&nbsp;KiB case, this translates to millions of <code>pread()</code> calls; at 64&nbsp;KiB,
                the same amount of data is covered by ~16× fewer calls.
            </p>

            <h3>Kernel page-cache lookup path (pseudocode)</h3>
            <div class="code-block">
<pre><code><span class="code-comment">// Extremely simplified view inspired by mm/filemap.c</span>

<span class="code-type">ssize_t</span> <span class="code-function">filemap_read</span>(<span class="code-keyword">struct</span> <span class="code-type">file</span> <span class="code-operator">*</span>file, <span class="code-keyword">struct</span> <span class="code-type">iov_iter</span> <span class="code-operator">*</span>iter) {
    <span class="code-keyword">struct</span> <span class="code-type">address_space</span> <span class="code-operator">*</span>mapping = file<span class="code-operator">-&gt;</span>f_mapping;
    <span class="code-type">pgoff_t</span> index = <span class="code-function">offset_to_index</span>(file<span class="code-operator">-&gt;</span>f_pos);

    <span class="code-keyword">while</span> (<span class="code-function">iov_iter_count</span>(iter) <span class="code-operator">&gt;</span> <span class="code-number">0</span>) {
        <span class="code-keyword">struct</span> <span class="code-type">folio_batch</span> fb;
        <span class="code-function">folio_batch_init</span>(<span class="code-operator">&amp;</span>fb);

        <span class="code-comment">// 1) Lookup or fetch folios from the page cache for this range</span>
        <span class="code-type">int</span> nr = <span class="code-function">filemap_get_pages</span>(mapping, file, <span class="code-operator">&amp;</span>index, <span class="code-operator">&amp;</span>fb);

        <span class="code-comment">// 2) Copy data from folios to user buffers</span>
        <span class="code-keyword">for</span> (i = <span class="code-number">0</span>; i <span class="code-operator">&lt;</span> nr; i<span class="code-operator">++</span>) {
            <span class="code-keyword">struct</span> <span class="code-type">folio</span> <span class="code-operator">*</span>f = fb.folios[i];
            <span class="code-function">copy_page_to_iter</span>(f, iter);   <span class="code-comment">// shows up hot in perf</span>
        }

        <span class="code-function">folio_batch_release</span>(<span class="code-operator">&amp;</span>fb);
    }
}</code></pre>
            </div>

            <p>
                <code>filemap_get_pages()</code> is where the XArray and folios come into play:
                it walks the page-cache index (<code>mapping-&gt;i_pages</code>), which is an
                <strong>XArray</strong> mapping file offsets to <strong>folio</strong> objects.
                Internally it uses helpers like <code>xas_load()</code> to look up cached folios and
                <code>__filemap_add_folio()</code> to insert new ones on a miss.
            </p>

            <div class="code-block">
<pre><code><span class="code-comment">// Rough mental model of how filemap_get_pages() behaves</span>

<span class="code-type">int</span> <span class="code-function">filemap_get_pages</span>(<span class="code-keyword">struct</span> <span class="code-type">address_space</span> <span class="code-operator">*</span>mapping,
                      <span class="code-keyword">struct</span> <span class="code-type">file</span> <span class="code-operator">*</span>file,
                      <span class="code-type">pgoff_t</span> <span class="code-operator">*</span>index,
                      <span class="code-keyword">struct</span> <span class="code-type">folio_batch</span> <span class="code-operator">*</span>fb) {
    <span class="code-keyword">struct</span> <span class="code-type">xa_state</span> xas;
    <span class="code-function">xas_init</span>(<span class="code-operator">&amp;</span>xas, <span class="code-operator">&amp;</span>mapping<span class="code-operator">-&gt;</span>i_pages, <span class="code-operator">*</span>index);

    <span class="code-keyword">while</span> (<span class="code-operator">!</span><span class="code-function">folio_batch_full</span>(fb)) {
        <span class="code-keyword">struct</span> <span class="code-type">folio</span> <span class="code-operator">*</span>f = <span class="code-function">xas_load</span>(<span class="code-operator">&amp;</span>xas);   <span class="code-comment">// XArray lookup</span>

        <span class="code-keyword">if</span> (<span class="code-operator">!</span>f) {
            <span class="code-comment">// Page cache miss: allocate folio and start I/O</span>
            f = <span class="code-function">page_cache_ra_unbounded</span>(mapping, file, <span class="code-operator">&amp;</span>xas);
        }

        <span class="code-function">folio_get</span>(f);
        <span class="code-function">folio_batch_add</span>(fb, f);

        (<span class="code-operator">*</span>index)<span class="code-operator">++</span>;
        <span class="code-function">xas_next</span>(<span class="code-operator">&amp;</span>xas);
    }

    <span class="code-keyword">return</span> <span class="code-function">folio_batch_count</span>(fb);
}</code></pre>
            </div>

            <p>
                In the 4&nbsp;KiB run, <code>perf report</code> showed functions like
                <code>filemap_get_pages</code>, <code>xas_load</code>, and
                <code>__filemap_add_folio</code> consuming a significant fraction of CPU cycles.
                That's the kernel doing <em>metadata work</em> to manage a huge number of tiny cache entries.
                At 64&nbsp;KiB, the same 8&nbsp;GiB is covered by far fewer folios, so these functions are called
                less often and their overhead drops.
            </p>

            <h3>Syscall perspective from <code>strace -c</code></h3>
            <p>
                On top of the kernel functions, <code>strace -c</code> showed that:
            </p>
            <ul>
                <li>
                    For 4&nbsp;KiB records, the benchmark issued millions of <code>read</code>/<code>pread</code>
                    syscalls, with total syscall time ≈ 6&nbsp;s.
                </li>
                <li>
                    For 64&nbsp;KiB records, syscall count dropped by ~16×, and total syscall time stayed closer
                    to ~1.5&nbsp;s.
                </li>
            </ul>
            <p>
                That difference in syscall volume is what drives the hot path in
                <code>copy_page_to_iter</code> and <code>_copy_to_iter</code> (data copying) and the page-cache
                management functions discussed above.
            </p>
        </div>

        <!-- Learnings & Next Steps -->
        <div class="content-block">
            <h2 class="section-title">Learnings &amp; Next Steps</h2>
            <p>
                The experiment confirmed the hypothesis from the book: making the FS cache “larger” in
                number of records (by shrinking record size) can hurt performance because the kernel spends
                more time walking and updating its page-cache metadata structures. On this machine, 64&nbsp;KiB
                records struck a good balance between syscall overhead and cache management work.
            </p>
            <p>
                Next, I'd like to:
            </p>
            <ul>
                <li>
                    Repeat the experiment with <code>mmap()</code> instead of <code>pread()</code> to compare
                    the fault-driven path against the explicit syscall path.
                </li>
                <li>
                    Try datasets larger than RAM to force real eviction behavior and watch how
                    <code>filemap_get_pages</code> and reclaim functions behave.
                </li>
                <li>
                    Extend the benchmark to test <code>O_DIRECT</code> I/O and see when bypassing the page cache
                    wins for database-like workloads.
                </li>
            </ul>
        </div>

        <!-- Resources -->
        <div class="content-block">
            <h2 class="section-title">Resources</h2>
            <ul class="resource-links">
                <li>Brendan Gregg’s writing on file system cache and negative tests (book + blog).</li>
                <li>
                    Linux kernel source: <code>mm/filemap.c</code>, <code>lib/xarray.c</code> for
                    <code>filemap_get_pages</code>, folios, and the XArray API.
                </li>
                <li>
                    <code>man 2 read</code>, <code>man 2 pread</code>, <code>man 2 mmap</code> for syscall semantics.
                </li>
                <li>
                    perf &amp; strace docs:
                    <code>man perf-stat</code>, <code>man perf-record</code>,
                    <code>man perf-report</code>, <code>man strace</code>.
                </li>
            </ul>
        </div>
    </section>
</main>
</body>

</html>